\section{Lecture 15: Multirate Signal Processing and Polyphase Representations}
%
\subsection{Changing the Sample Rate By a Non-Integer Factor}
%
Previously, we discussed downsampling and upsampling by an integer rate.
For downsampling by a factor of $M$, we first prefiltered with a low-pass
filter with cutoff frequency $\pi/M$ and gain of $1$ to prevent aliasing,
then downsampled by $M$. For upsampling by a factor of $L$, we padded our
signal with $L-1$ zeroes between each of the original samples, then filtered
with a low-pass filter with cutoff frequency $\pi/L$ and gain of $L$
to interpolate.\\
%
Upsampling or downsampling by integer rates is rather restrictive; there are
many situations where we'd like to use non-integer rates. Consider changing
the sampling rate by the non-integer factor $\tau$, which can be written as
the quotient of two integers, $M/L$. We could in principle make some combination
of upsampling by $L$ and downsampling by $M$ to achieve this. So,
%
\begin{displaymath}
  x[n] \longrightarrow \boxed{\uparrow L}
  \longrightarrow \boxed{H_\mathrm{int}(\omega)}
  \longrightarrow \boxed{H_\mathrm{pre}(\omega)}
  \longrightarrow \boxed{\downarrow M}
  \longrightarrow x_\tau[n] \,.
\end{displaymath}
%
Note that the interpolating filter and prefilter can be condensed into a single
filter; in the frequency domain, we need simply multiply them together, which
results in a single low-pass filter whose cutoff frequency is dictated by
the smallest cutoff frequency of the other two,
%
\begin{displaymath}
  H_\tau(\omega) = H_\mathrm{int}(\omega)H_\mathrm{pre}(\omega)
  \quad \omega_c = \min\left(\frac{\pi}{M},\frac{\pi}{L}\right) \,.
\end{displaymath}
%
If $M>L$, we have a net reduction in sampling rate, meaning $H_\tau(\omega)$
is $H_\mathrm{pre}(\omega)$, and we need to prevent possible aliasing. If
$L>M$, we have a net increase in sampling rate, meaning $H_\tau(\omega)$ is
$H_\mathrm{int}(\omega)$, and we need to interpolate the signal. To give some
numbers, assume $\tau = 1.2$, then we need to upsample by a factor of $5$
and downsample by a factor of $6$, meaning we need to use
$H_\mathrm{pre}(\omega)$.

\subsection{The Noble Identities}
%
This strategy is problematic. Consider $\tau = 1.01$, where $M = 101$ and
$L = 100$, which seems rather wasteful since we're implementing large intermediate
changes in rate to retrieve a signal which is close to the original rate. We then
ask whether this process can be done more efficiently, to which the answer is of
course yes, and is referred to as \textbf{multirate signal processing}.
%
\begin{iden}[\textbf{The Noble Identity for Decimation}]
  The processes
  %
  \begin{displaymath}
    x[n] \longrightarrow \boxed{\downarrow M} \longrightarrow x_a[n]
    \longrightarrow \boxed{H(z)} \longrightarrow y_a[n]
  \end{displaymath}
  %
  and
  %
  \begin{displaymath}
    x[n] \longrightarrow \boxed{H\left(z^M\right)} \longrightarrow x_b[n]
    \longrightarrow \boxed{\downarrow M} \longrightarrow y_b[n]
  \end{displaymath}
  %
  are equivalent.\\

  Let's consider the meaning of $H(z^M)$ for a little insight:
  %
  \begin{displaymath}
    H(z^M) = \infsum{n} h[n]z^{-Mn} = \mathscr{Z}(h_e[n]) \,,
  \end{displaymath}
  %
  where $h_e[n]$ is a zero-padded version of $h[n]$, with $M-1$ zeroes between
  each sample. Recall that to transform between the $\omega$ and $z$ domains,
  we have the relationship
  %
  \begin{displaymath}
    X(\omega) = \left. X(z) \right|_{z = \ex{\im\omega}} \,.
  \end{displaymath}
  %
  Consequently, we can write the expression
  %
  \begin{displaymath}
    X_b(\omega) = H\left(z^M\right)X(\omega) = H\left(\ex{\im\omega M}\right)X(\omega)
    = H(\omega M) X(\omega)
  \end{displaymath}
  %
  since
  %
  \begin{displaymath}
    \left.H\left(z^M\right)\right|_{z=\ex{\im\omega}}
    = \infsum x[n]\ex{\im\omega nM} = H(\omega M) \,.
  \end{displaymath}
  %
  We saw in the previous lecture that the downsampling block of $X_b(\omega)$ results in
  %
  \begin{displaymath}
    Y_b(\omega) = \frac{1}{M}\sum_{m=0}^{M-1}X\left(\frac{\omega - 2\pi m}{M}\right)
    H(\omega - 2\pi m)
    = H(\omega)\frac{1}{M}\sum_{m=0}^{M-1} X\left(\frac{\omega - 2\pi m}{M}\right) \,,
  \end{displaymath}
  %
  since $H(\omega)$ is $2\pi$-periodic. But this is simply $H(\omega)$ multiplied by
  the result from having first downsampled by $X(\omega)$ by $M$, and consequently we
  have proven that $Y_b(\omega) = Y_a(\omega)$.
\end{iden}
%
\begin{iden}[\textbf{The Noble Identity for Interpolation}]
  The processes
  %
  \begin{displaymath}
    x[n] \longrightarrow \boxed{\uparrow L} \longrightarrow x_a[n]
    \longrightarrow \boxed{H(z)} \longrightarrow y_a[n]
  \end{displaymath}
  %
  and
  %
  \begin{displaymath}
    x[n] \longrightarrow \boxed{H\left(z^L\right)} \longrightarrow x_b[n]
    \longrightarrow \boxed{\uparrow L} \longrightarrow y_b[n]
  \end{displaymath}
  %
  are equivalent.\\

  Recall that upsampling shrinks the spectrum in the frequency domain by a factor
  $L$. Then
  %
  \begin{displaymath}
    Y_a(\omega) = X_a(\omega L) = X(\omega L)H(\omega L) \,,
  \end{displaymath}
  %
  and similarly
  %
  \begin{displaymath}
    X_b(\omega) = X(\omega L) \,,
  \end{displaymath}
  %
  meaning
  %
  \begin{displaymath}
    Y_b(\omega) = X(\omega L)H(\omega L) \,,
  \end{displaymath}
  %
  and we've proven that $Y_a(\omega) = Y_b(\omega)$. This seemingly innocent looking
  interchange has important computational consequences. If we upsample first, we increase
  our signal length by a factor of $L$, most of the elements being zero. This then
  results in a lot of redundant computation by the subsequent filter which has to
  process the signal in serial. In contrast, if we filter first, we have a significantly
  reduced computational overhead, which can later be upsampled.
\end{iden}

\subsection{The Polyphase Decomposition}
%
Consider some signal $h[n]$, an example of which is depicted in Figure QQ. We can think
of this sum as a sum of simpler signals; $h[n]$ is decomposed into a sum of $M$
subsequences $h_k[n]$, defined as follows,
%
\begin{displaymath}
  h_k[n] = h[k + nM] \,,
\end{displaymath}
%
which is the original sequence delayed by $k$ units and zero-padded with $M-1$ zeroes
between each of the originally sampled points. This is simplest to understand pictorially;
for $M=3$, pane (b) of Figure QQ shows the decomposition. We see that the original
signal is simply the summation of these subsequences,
%
\begin{displaymath}
  h[n] = \sum_{k=0}^{M-1}h_k[n-k] \,.
\end{displaymath}
%
Let $e_k[n]$ denote a corresponding $h_k[n]$ whose zero-padding has been removed, i.e.
$e_k[n] = h[nM + k] = h_k[nM]$. These are termed the polyphase components of $h[n]$.
Now, consider the Z-transform of $h[n]$ and split it into $M$ distinct sums,
%
\begin{align*}
  H(z) &= \infsum{k} h[k]z^{-k}
  = \infsum{\ell}h[\ell M]z^{-\ell M} + \infsum{\ell}h[\ell M+1]z^{-(\ell M + 1)} + \hdots \\
  &= \sum_{k=0}^{M-1}\infsum{\ell}h[\ell M + k]z^{-(\ell M + k)}
  = \sum_{k=0}^{M-1}z^{-k}\infsum{\ell}h[\ell M + k]z^{-\ell M} \\
  &= \sum_{k=0}^{M-1}z^{-k} \infsum{\ell}e_k[\ell]z^{-\ell M}
  = \sum_{k=0}^{M-1}z^{-k}\mathscr{E}_k\left(z^M\right) \,,
\end{align*}
%
where $\mathscr{E}_k\left(z^M\right)$ is the Z-transform of the $k\th$ polyphase
component shifted by $M$. Using this as a transfer function, we see that the
$k\th$ term in this summation is the input delayed by $k$ (since
$X(z)z^{-k} \Longleftrightarrow x[n-k]$) multiplied by the $k\th$ polyphase
component shifted by $M$, $\mathscr{E}\left(z^M\right)$. Each of these terms
is then summed to yield the output, $Y(z)$. This is the \textrm{polyphase realisation}
of $H(z)$.\\

\subsubsection{Polyphase Downsampling}
%
Now, let's think again about downsampling, where we prefiltered to prevent aliasing
then downsampled the signal. By using the polyphase realisation of our prefilter,
we can implement this as depicted in pane (a) of Figure QQ. The signal is passed
through $\mathscr{E}_0\left(z^M\right)$ on the first branch; on the second branch, it
is first delayed by the $z^{-1}$ block, then passed through $\mathscr{E}_1\left(z^M\right)$,
at which point it is summed with the result from the first branch. This is precisely
what the first two terms in the summation above represent, and proceeds up to
$\mathscr{E}_{M-1}\left(z^M\right)$. Once the summation has been completed, the
downsampling $M$ completes the process and we're left with our downsampled signal.\\
%
The polyphase realisation of the prefilter doesn't seem to have accomplished much;
consider a filter of length 400 that we downsample by a factor of 50; there are
then 50 polyphase components of the filter, each of length 8. For every value of the
input, this means we need to complete $8\times 50 = 400$ multiplications, exactly
the same had we kept the prefilter in its original form, albeit we've expressed
some parallelism. But we recall that the Noble identity for decimation allows
us to interchange the order of filtering and downsampling while changing the argument
of the filter from $z^M$ to $z$. This means we can implement the polyphase downsampling
as depicted in pane (b) of Figure QQ. Now since we downsample first, we pass $M$
times fewer values through each of the polyphase components, and our complexity
per input value becomes $400 / 8 = 50$ multiplications. So, the polyphase
downsampling lets us save a factor of $M$ in computational complexity. This is
a useful result since the larger the downsampling factor, the more data points
we're throwing away after having expended computational effort in prefiltering.
Now the complexity is independent of the downsampling factor, meaning we
don't waste effort.
